devices: 1  # Number of GPU used for training
name: "train"  # a name for the run
accelerator: gpu  # cpu, gpu, tpu, ... (see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#accelerator for available accelerators)
distributed_backend: ddp  # see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#strategy
seed: 0  # seed of the run
fast_dev_run: false  # whether to start the training in fast_dev_run mode (see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run)
max_epochs: -1  # number of trained epochs. If -1, will use max_steps instead
max_steps: 100000  # number of trained steps

simple_shapes_path: ???  # path to root folder of the dataset

current_dataset: "shapes"  # dataset to use. For now, only "shapes"

img_size: 32  # width and height of images in the dataset

n_validation_examples: 32  # number of validation examples for the images logged (only for text and images, losses will use the whole val dataset)

checkpoints_dir: "../checkpoints"  # path to save checkpoints artifacts
resume_from_checkpoint: null  # path to a checkpoint to continue training. For train_lm path to the LM, for train_vae path to the VAE...
logger_resume_id: null  # if not null, will resume the logged run (such as wandb run) with this id.
progress_bar: true  # whether to show the progress bar

checkpoint: null  # path to the GW model for evaluation or path to folder containing the model. If folder, the best model will be loaded
# When loading checkpoints (checkpoint, or resume_from_checkpoint), you can load a model from
# a local server by using the following parameters:
#checkpoint:
#  load_from: remote
#  remote_server: "example.com"
#  remote_user: "user"
#  remote_password: "pass"
#  remote_checkpoint_path: "/path/to/checkpoint"  # can be folder or file. If folder, the latest model will be loaded.
#  local_path: "../local/path/to/save/checkpoint"  # must be folder

datasets:
  shapes:
    n_train_examples: 1_000_000
    n_val_examples: 100_000
    n_test_examples: 100_000
    min_scale: 7
    max_scale: 14
    min_lightness: 46
    max_lightness: 255
    shapes_color_range:
        - [-12, 64]
        - [52, 128]
        - [116, 191]
        # - [65, 191]
        # - [129, 231]
        # - [192, 295]


slurm_id: "${oc.env:SLURM_JOB_ID,null}"  # id of the job
slurm: # slurm parameters if started on slurm with the slurm/launch.py command
  slurm: # slurm parameters (see https://slurm.schedmd.com/sbatch.html#SECTION_OPTIONS)
    "-N": 1
    "-J": ${name}
    "--gres": "gpu:${devices}"
    "--time": "12:00:00"
    "--mem": 5000
  script: train  # which script to start
  command: null
  # Before scheduling the run, the project code and config will be copied to a working directory for reproducibility.
  # path to the folder where all copies will be made.
  run_work_directory: null
  pre_modules: null  # modules to load before starting the job
  run_modules: null # modules to load when the job has started
  python_environment: null  # path to the python environment to use or name of the conda environment to use
  grid_search: null
  grid_search_exclude: null

dataloader:
  num_workers: 0

run_name: "v${_code_version}_${slurm.slurm.-J}_slug=${coef_slug:}_N=${global_workspace.prop_labelled_images}"  # name of the run used in the loggers.
# ${cloef_slug:} is replaced by the coef_slug value depending on coefficient values.
loggers: [ ]
#  - logger: WandbLogger
#    save_images: true
#    save_last_images: true  # whether to save the images generated after the last epoch.
#    save_last_tables: true
#    save_tables: true
#    watch_model: true
#    args:
#      save_dir: "path/to/wandb/log/dir"
#      project: "bimGW"
# Additional args to wandb.init
#  - logger: TensorBoardLogger
#    save_images: true
#    args:
#      save_dir: "../logs/tensor_board_logger"
#  - logger: CSVLogger
#    save_images: true
#    args:
#      save_dir: "../logs/csv_logger"
#  - logger: NeptuneLogger
#    save_images: true
#    args:
#      project: null
#      api_key: null
#      mode: async
#      run: null


domain_loader: # Additional domain loader parameters.
  t: # for text domain loader
    bert_latents: bert-base-uncased.npy  # path to the extracted BERT features
    pca_dim: 768  # dimension of the PCA projection of the BERT features. If 768, no PCA is applied
  attr:
    use_unpaired: true  # whether to load the unpaired attribute from the dataset


# =====
# Visualization parameters
# =====
visualization:
  fg_color: "black"  # plot foreground color
  bg_color: "white"  # plot background color
  font_size: 12
  font_size_title: 14
  line_width: 2  # curve line width

  mix_loss_coefficients: # mix_loss is the weighted average of translation and contrastive
    translation: 1.
    contrastive: 0.1

  loss_definitions: { }
  # Example loss definitions
  # loss_definitions:
  #   translation:  # name of the loss
  #     # list of keys to average over
  #     - "val/in_dist/translation_loss.min"
  #   contrastive: 
  #     - "val/in_dist/contrastive_loss.min"
  #   cycles: 
  #     - "val/in_dist/cycles_loss.min"
  #   demi_cycles: 
  #     - "val/in_dist/demi_cycles_loss.min"
  #   mix_loss:
  #     - "val/in_dist/translation_loss.min"

  argmin_over: "mix_loss"  # which loss definition to use for parameter selection

  x_axis: prop_labelled  # prop_labelled (N) or prop_available (N+M)

  # By default, slug is computed by looking at the loss coefficients.
  # If a coefficient is > 0, then "+loss_name" is added to slug.
  # For example, the slug for translation > 0 and demi-cycles > 0 is "tr+dcy"
  # You can add values to slug according to some conditions
  additional_slug_conds: []
  # Example of additional slug conditions
  # Here, we will have "+baseline" added at the end of the slug
  # if "parameters/some_params" == "42", and "+random" if it equals "0"
  # additional_slug_conds:
  #   - slug_value: "+baseline"  # what to add to the slug (at the end)
  #     key: "parameters/some_params"
  #     eq: "42"
  #   - slug_value: "+random"  # what to add to the slug (at the end)
  #     key: "parameters/some_params"
  #     eq: "0"
  

  saved_figure_path: "../../data/plots"  # where to save figures
  total_num_examples: 500_000  # used for the x-axis to go from proportions to absolute number of matched examples
  legend:
    num_columns: 2

  wspace: null  # horizontal spacing between axes
  hspace: 0.0625  # vertical spacing between axes

  figures:
    - title: "Global results"
      selected_losses:
        translation:
          label: Translation losses
          # should be *in this order* separated by a "+": tr, cont, dcy, then cy.
          # To not include one loss component, just remove it from the list.
          # For example: "tr+cont" is valid, "cont+tr" is not.
          curves: 
            - "tr"
            - "tr+cy"
            - "tr+cont"
            - "tr+cont+dcy+cy"
        contrastive:
          label: Contrastive losses
          curves:
            - "tr"
            - "tr+cy"
            - "tr+cont"
            - "tr+cont+dcy+cy"

      transpose_fig: false  # if true, will transpose columns and rows

      cols:
        - label: "Vision — Proto-language"
          load_from: wandb  # among wandb or csv
          wandb_entity_project: null  # If load_from is wandb, entity/project-name
          wandb_filter: null  # If load_from is wandb, filter with MongoDB syntax (see https://docs.wandb.ai/guides/track/public-api-guide#filter-runs)
          # For example, load jobs with a certain slurm job name and have a finished state:
          # wandb_filter: 
          #   "$and": 
          #      - "config.parameters/slurm/slurm/-J": "slurm_job_name"
          #      - 'State': "finished"
          # 
          csv_path: null  # If load_from is csv, path to csv file containing the run to display
        - label: "Vision — Natural Language"
          load_from: wandb  # similar to attributes
          wandb_entity_project: null
          wandb_filter: null
          csv_path: null
          annotations:
            - y: 0.6
              text_yshift: 0.03  # shifts the text vertically
              loss: translation
              curve_start: "tr+cont+dcy+cy"
              curve_end: "tr"
    - title: "P1"
      # should be *in this order* separated by a "+": tr, cont, dcy, then cy.
      # To not include one loss component, just remove it from the list.
      # For example: "tr+cont" is valid, "cont+tr" is not.
      selected_losses:
        cycles:
          label: Cycle losses
          curves:
            - "tr"
            - "tr+cy"
            - "tr+cont"
            - "tr+cont+dcy+cy"
        demi_cycles:
          label: Demi-cycle losses
          curves:
            - "tr"
            - "tr+cy"
            - "tr+cont"
            - "tr+cont+dcy+cy"

      transpose_fig: false

      cols:
        - label: "Vision — Proto-language"
          load_from: wandb  # among wandb or csv
          wandb_entity_project: null  # If load_from is wandb, entity/project-name
          wandb_filter: null  # If load_from is wandb, filter with MongoDB syntax (see https://docs.wandb.ai/guides/track/public-api-guide#filter-runs)
          csv_path: null  # If load_from is csv, path to csv file containing the run to display
        - label: "Vision — Natural Language"
          load_from: wandb  # similar to attributes
          wandb_entity_project: null
          wandb_filter: null
          csv_path: null

# =====
# VAE parameters
# =====
vae:
  batch_size: 512

  beta: 0.1
  z_size: 12  # size of the VAE latent space
  ae_size: 512  # hidden size dimension
  type: "beta"  # in "beta", "sigma", "optimal_sigma"

  n_fid_samples: 1000  # num of samples to compute FID

  early_stopping_patience: null  # no early stopping

  optim:
    lr: 3e-4
    weight_decay: 1e-5
  scheduler:
    step: 100
    gamma: 0.9

# =====
# Language model parameters
# =====
lm:
  z_size: 24
  hidden_size: 64
  beta: 0.1

  train_vae: true
  train_attr_decoders: false
  optimize_vae_with_attr_regression: false

  coef_attr_loss: 100_000
  coef_vae_loss: 1

  batch_size: 1024
  n_validation_examples: 32
  early_stopping_patience: null

  optim:
    lr: 1e-4
    weight_decay: 3e-5
  scheduler:
    step: 200
    gamma: 0.0

# =====
# GW parameters
# =====
global_workspace:
  batch_size: 128
  selected_domains: [ ]  # domains to load dict where keys are names used in the loggers, and values are among "v" for vision, "t" for text and "attr" for attributes
  sync_uses_whole_dataset: false  # dataset has 1 000 000 examples to test training VAE and GW on different splits. If true will use the 1 000 000 examples for training, otherwise half. Default to half.

  use_pre_saved: false
  load_pre_saved_latents: null

  split_ood: false  # whether to remove a out-of-distribution split in the training set.
  ood_hole_attrs: 6  # number of attrs that are selected to make the ood hole. The bigger this value is, the smaller the hole is.
  ood_seed: 0
  ood_idx_domain: 0  # in which domain are the ood split
  ood_create_new_examples: false  # will create new ood examples for validation / test
  ood_folder: null
  bert_path: "bert-base-uncased"

  z_size: 12  # size of the GW latent space

  hidden_size:
    encoder: { "t": 128, "attr": 128, "v": 64 }
    decoder: { "t": 128, "attr": 128, "v": 64 }

  n_layers:
    encoder: { "t": 2, "attr": 1, "v": 2 }
    decoder: { "t": 0, "attr": 0, "v": 0 }
    decoder_head: { "t": 2, "attr": 2, "v": 2 }

  prop_labelled_images: 1.  # proportion of the allowed classes to be labelled.
  prop_available_images: 1.  # proportion of available images used in cycles and demi-cycles.
  remove_sync_domains: null
  vae_checkpoint: null
  lm_checkpoint: null

  early_stopping_patience: 20
  optim:
    lr:
      encoders: 1e-4
      decoders: 3e-4
      supervised_multiplier: 1.
      unsupervised_multiplier: 1.
    weight_decay: 5e-5

  scheduler:
    # "fixed"/"adaptive".
    # - Fixed will use the following values.
    # - Adaptive will change the values based on the prop_labelled_image setting.
    #   The given values are the one used if prop_labelled_images is set to 1.
    mode: fixed
    interval: epoch
    step: 300
    gamma: 0.5

losses:
  schedules: null
  coefs:
    demi_cycles: 1.
    cycles: 1.
    supervision: null  # Deprecated. Here for backward compatibility, use translation instead. If not null, will override the value in translation.
    translation: 1.
    cosine: 0.
    contrastive: 1.

# =====
# Odd-one-out experiment parameters
# =====
odd_image:
  batch_size: 512
  encoder:
    use_dist: false  # whether to classify using pairwise distances. 
    selected_id: null
    path: null  # path to checkpoint. If null, will train a new encoder. If "random" will use a random encoder.
    # If "identity", will not use the encoder.
    # If a path is give, it can use ${odd_image.encoder.selected_slurm_id} which is automatically
    # set using the load_from below.
    # For example
    # path: "path/to/checkpoints/${odd_image.encoder.selected_id}/last.ckpt"
    # If the path leads to a folder, the last checkpoint is automatically loaded.

    load_from: null  # similar to the visualization. Can be null, "wandb" or "csv".
    # If "wandb", will override odd_image.encoder.selected_id from a run selected from wandb.
    # If "csv", will override from a run from the csv file given in odd_image.encoder.csv_path.
    # If null, will not overwrite odd_image.encoder.selected_id.
    wandb_entity_project: null
    wandb_filter: null
    csv_path: null
    selected_id_key: "Name"  # from which column the id is taken in the loaded run.

  checkpoint: # params for loading pretrained classifier
    selected_id: null
    load_from: null  # similar to the visualization. Can be null, "wandb" or "csv".
    # If "wandb", will override odd_image.encoder.selected_id from a run selected from wandb.
    # If "csv", will override from a run from the csv file given in odd_image.encoder.csv_path.
    # If null, will not overwrite odd_image.encoder.selected_id.
    wandb_entity_project: null
    wandb_filter: null
    csv_path: null

  # If load from is not null, which items to select from the loaded runs?
  select_row_from_index: null  # If this is not null, will select the corresponding item.
  select_row_from_current_coefficients: false  # if select_row_from_index is null and this is true, will select the run which uses the same loss coefficients as currently set.

  optimizer:
    lr: 3e-4
    weight_decay: 1e-4

downstream:
  unpaired_cls:
    random_regressor: false
    checkpoint: "path/to/checkpoint"

    optimizer:
      lr: 1e-3
      weight_decay: 1e-5

_code_version: master
