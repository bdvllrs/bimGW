devices: 1  # Number of GPU used for training
accelerator: gpu  # cpu, gpu, tpu, ... (see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#accelerator for available accelerators)
distributed_backend: ddp  # see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#strategy
seed: 0  # seed of the run
debug: false
fast_dev_run: false
max_epochs: -1  # number of trained epochs. If -1, will use max_steps instead
max_steps: 100000  # number of trained steps

simple_shapes_path: ???  # path to root folder of the dataset

visual_dataset: "shapes"  # dataset to use. For now, only "shapes"

img_size: 32  # size of the images in the dataset

n_validation_examples: 32  # number of validation examples for the images logged (losses will use the whole val dataset)

checkpoints_dir: "../checkpoints"  # path to save checkpoints artifacts
resume_from_checkpoint: null  # path to a checkpoint to continue training

slurm:  # slurm parameters if started on slurm with the slurm/launch.py command
  script: train  # which script to start
  slurm:
    "-N": 1
    "--gres": "gpu:${devices}"
    "--time": "12:00:00"
    "--mem": 5000

fetchers:
  t:
    bert_latents: bert-base-uncased_simple.npy

# =====
# VAE parameters
# =====
vae:
  batch_size: 128
  data_augmentation: false  # whether to use data_augmentation on the images

  beta: 0.5
  z_size: 12  # size of the VAE latent space
  ae_size: 512  # hidden size dimension
  type: "beta"  # in "beta", "sigma", "optimal_sigma"

  n_FID_samples: 1000  # num of samples to compute FID

  early_stopping_patience: null  # no early stopping

  optim:
    lr: 1e-4
    weight_decay: 3e-5
  scheduler:
    step: 20
    gamma: 0.5

# =====
# Language model parameters
# =====
lm:
  z_size: 12  # size of the latent space after projection

  batch_size: 128
  n_validation_examples: 32
  early_stopping_patience: null

  optim:
    lr: 1e-4
    weight_decay: 3e-5
  scheduler:
    step: 10
    gamma: 0.1

# =====
# GW parameters
# =====
global_workspace:
  monitor_grad_norms: false  # if true, logs gradients (for debug, training is a bit slower)

  batch_size: 128
  data_augmentation: false
  selected_domains: {}  # domains to load dict where keys are names used in the loggers, and values are among "v" for vision, "t" for text and "attr" for attributes
  sync_uses_whole_dataset: false  # dataset has 1 000 000 examples to test training VAE and GW on different splits. If true will use the 1 000 000 examples for training, otherwise half. Default to half.

  use_pre_saved: false
  load_pre_saved_latents: null

  split_ood: false  # whether to remove a out-of-distribution split in the training set.
  bert_path: "bert-base-uncased"

  z_size: 12  # size of the GW latent space

  hidden_size:
    encoder: {"t": 512, "attr": 32, "v": 32}
    decoder: {"t": 512, "attr": 32, "v": 32}

  n_layers:
    encoder: {"t": 1, "attr": 1, "v": 2}
    decoder: {"t": 0, "attr": 0, "v": 0}
    decoder_head: {"t": 2, "attr": 2, "v": 2}

  prop_labelled_images: 1.  # of the allowed classes to be labelled.
  remove_sync_domains: null
  classes_labelled_images: null  # null is all classes
  vae_checkpoint: null
  lm_checkpoint: null
  vae_mmd_loss_coef: 0.
  vae_kl_loss_coef: 0.

  early_stopping_patience: 10
  optim:
    lr:
      encoders: 1e-4
      decoders: 1e-4
      supervised_multiplier: 1.
      unsupervised_multiplier: 1.
    weight_decay: 3e-5

  scheduler:
    # "fixed"/"adaptive".
    # - Fixed will use the following values.
    # - Adaptive will change the values based on the prop_labelled_image setting.
    #   The given values are the one used if prop_labelled_images is set to 1.
    mode: fixed
    interval: epoch
    step: 20
    gamma: 0.5

losses:
  schedules: null
  coefs:
    demi_cycles: 0.
    cycles: 0.
    supervision: 1.
    cosine: 0.
    contrastive: 1.3

gensim_model_path: null
word_embeddings: null

dataloader:
  num_workers: 0

loggers:
  - logger: WandbLogger
    save_images: true
    watch_model: true
    args:
      save_dir: "path/to/wandb/log/dir"
      project: "bim-gw"
      # Additional args to wandb.init
#  - logger: TensorBoardLogger
#    save_images: true
#    args:
#      save_dir: "../logs/tensor_board_logger"
#  - logger: CSVLogger
#    save_images: true
#    args:
#      save_dir: "../logs/csv_logger"
#  - logger: NeptuneLogger
#    save_images: true
#    args:
#      project: null
#      api_key: null
#      mode: async
#      run: null

# =====
# Odd-one-out experiment parameters
# =====
odd_image:
  batch_size: 512
  encoder_path: null  # path to checkpoint
  csv_ids: null
  csv_row: null
  resume_csv: null

  optimizer:
    lr: 1e-3
    weight_decay: 1e-5

_code_version: master
