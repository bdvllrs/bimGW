devices: 1  # Number of GPU used for training
name: "train"  # a name for the run
accelerator: gpu  # cpu, gpu, tpu, ... (see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#accelerator for available accelerators)
distributed_backend: ddp  # see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#strategy
seed: 0  # seed of the run
fast_dev_run: false  # whether to start the training in fast_dev_run mode (see https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run)
max_epochs: -1  # number of trained epochs. If -1, will use max_steps instead
max_steps: 100000  # number of trained steps

simple_shapes_path: ???  # path to root folder of the dataset

current_dataset: "shapes"  # dataset to use. For now, only "shapes"

img_size: 32  # width and height of images in the dataset

n_validation_examples: 32  # number of validation examples for the images logged (only for text and images, losses will use the whole val dataset)

checkpoints_dir: "../checkpoints"  # path to save checkpoints artifacts
resume_from_checkpoint: null  # path to a checkpoint to continue training. For train_lm path to the LM, for train_vae path to the VAE...
logger_resume_id: null  # if not null, will resume the logged run (such as wandb run) with this id.

checkpoint: null  # path to the GW model for evaluation or path to folder containing the model. If folder, the best model will be loaded
# When loading checkpoints (checkpoint, or resume_from_checkpoint), you can load a model from
# a local server by using the following parameters:
#checkpoint:
#  load_from: remote
#  remote_server: "example.com"
#  remote_user: "user"
#  remote_password: "pass"
#  remote_checkpoint_path: "/path/to/checkpoint"  # can be folder or file. If folder, the latest model will be loaded.
#  local_path: "../local/path/to/save/checkpoint"  # must be folder

slurm_id: "${oc.env:SLURM_JOB_ID,null}"  # id of the job
slurm: # slurm parameters if started on slurm with the slurm/launch.py command
  slurm: # slurm parameters (see https://slurm.schedmd.com/sbatch.html#SECTION_OPTIONS)
    "-N": 1
    "-J": ${name}
    "--gres": "gpu:${devices}"
    "--time": "12:00:00"
    "--mem": 5000
  script: train  # which script to start
  command: null
  # Before scheduling the run, the project code and config will be copied to a working directory for reproducibility.
  # path to the folder where all copies will be made.
  run_work_directory: null
  pre_modules: null  # modules to load before starting the job
  run_modules: null # modules to load when the job has started
  python_environment: null  # path to the python environment to use or name of the conda environment to use
  grid_search: null

dataloader:
  num_workers: 0

run_name: "${slurm.slurm.-J}_slug=${coef_slug:}_N=${global_workspace.prop_labelled_images}"  # name of the run used in the loggers.
# ${cloef_slug:} is replaced by the coef_slug value depending on coefficient values.
loggers: [ ]
#  - logger: WandbLogger
#    save_images: true
#    save_last_images: true  # whether to save the images generated after the last epoch.
#    save_last_tables: true
#    save_tables: true
#    watch_model: true
#    args:
#      save_dir: "path/to/wandb/log/dir"
#      project: "bimGW"
# Additional args to wandb.init
#  - logger: TensorBoardLogger
#    save_images: true
#    args:
#      save_dir: "../logs/tensor_board_logger"
#  - logger: CSVLogger
#    save_images: true
#    args:
#      save_dir: "../logs/csv_logger"
#  - logger: NeptuneLogger
#    save_images: true
#    args:
#      project: null
#      api_key: null
#      mode: async
#      run: null


fetchers: # Additional fetcher parameters.
  t: # for text fetcher
    bert_latents: bert-base-uncased.npy  # path to the extracted BERT features
    pca_dim: 768  # dimension of the PCA projection of the BERT features. If 768, no PCA is applied
  attr:
    use_unpaired: true  # whether to load the unpaired attribute from the dataset


# =====
# Visualization parameters
# =====
visualization:
  fg_color: "black"  # plot foreground color
  bg_color: "white"  # plot background color
  font_size: 12
  font_size_title: 14
  line_width: 2  # curve line width

  gw_results:
    saved_figure_path: "../../data/plots"  # where to save figures
    mix_loss_coefficients: # mix_loss is the weighted average of translation and contrastive
      translation: 1.
      contrastive: 0.1
    total_num_examples: 500_000  # used for the x-axis to go from proportions to absolute number of matched examples
    legend:
      num_columns: 2
    axes:
      # should be *in this order* separated by a "+": tr, cont, dcy, then cy. To not include one loss component, just remove it from the list.
      # For example: "tr+cont" is valid, "cont+tr" is not.
      selected_curves: [ "tr", "tr+cy", "tr+cont", "tr+cont+dcy+cy" ]
      attributes:
        load_from: wandb  # among wandb or csv
        wandb_entity_project: null  # If load_from is wandb, entity/project-name
        wandb_filter: null  # If load_from is wandb, filter with MongoDB syntax (see https://docs.wandb.ai/guides/track/public-api-guide#filter-runs)
        # For example, load jobs with a certain slurm job name and have a finished state:
        # wandb_filter: {
        #   "$and": [
        #     { "config.parameters/slurm/slurm/-J": "slurm_job_name" },
        #     'State': "finished"
        #   ]
        # }
        csv_path: null  # If load_from is csv, path to csv file containing the run to display
      text:
        load_from: wandb  # similar to attributes
        wandb_entity_project: null
        wandb_filter: null
        csv_path: null

    loss_definitions:
      translation: [ "val/in_dist/translation_loss.min" ]  # list of the logged losses to average for translation
      contrastive: [ "val/in_dist/contrastive_loss.min" ]  # similar for contrastive


# =====
# VAE parameters
# =====
vae:
  batch_size: 512

  beta: 0.1
  z_size: 12  # size of the VAE latent space
  ae_size: 512  # hidden size dimension
  type: "beta"  # in "beta", "sigma", "optimal_sigma"

  n_fid_samples: 1000  # num of samples to compute FID

  early_stopping_patience: null  # no early stopping

  optim:
    lr: 3e-4
    weight_decay: 1e-5
  scheduler:
    step: 100
    gamma: 0.9

# =====
# Language model parameters
# =====
lm:
  z_size: 24
  hidden_size: 64
  beta: 0.1

  train_vae: true
  train_attr_decoders: false
  optimize_vae_with_attr_regression: false

  coef_attr_loss: 100_000
  coef_vae_loss: 1

  batch_size: 1024
  n_validation_examples: 32
  early_stopping_patience: null

  optim:
    lr: 1e-4
    weight_decay: 3e-5
  scheduler:
    step: 200
    gamma: 0.0

# =====
# GW parameters
# =====
global_workspace:
  batch_size: 128
  selected_domains: [ ]  # domains to load dict where keys are names used in the loggers, and values are among "v" for vision, "t" for text and "attr" for attributes
  sync_uses_whole_dataset: false  # dataset has 1 000 000 examples to test training VAE and GW on different splits. If true will use the 1 000 000 examples for training, otherwise half. Default to half.

  use_pre_saved: false
  load_pre_saved_latents: null

  split_ood: false  # whether to remove a out-of-distribution split in the training set.
  bert_path: "bert-base-uncased"

  z_size: 12  # size of the GW latent space

  hidden_size:
    encoder: { "t": 128, "attr": 128, "v": 64 }
    decoder: { "t": 128, "attr": 128, "v": 64 }

  n_layers:
    encoder: { "t": 2, "attr": 1, "v": 2 }
    decoder: { "t": 0, "attr": 0, "v": 0 }
    decoder_head: { "t": 2, "attr": 2, "v": 2 }

  prop_labelled_images: 1.  # proportion of the allowed classes to be labelled.
  prop_available_images: 1.  # proportion of available images used in cycles and demi-cycles.
  remove_sync_domains: null
  vae_checkpoint: null
  lm_checkpoint: null

  early_stopping_patience: 20
  optim:
    lr:
      encoders: 1e-4
      decoders: 3e-4
      supervised_multiplier: 1.
      unsupervised_multiplier: 1.
    weight_decay: 5e-5

  scheduler:
    # "fixed"/"adaptive".
    # - Fixed will use the following values.
    # - Adaptive will change the values based on the prop_labelled_image setting.
    #   The given values are the one used if prop_labelled_images is set to 1.
    mode: fixed
    interval: epoch
    step: 300
    gamma: 0.5

losses:
  schedules: null
  coefs:
    demi_cycles: 1.
    cycles: 1.
    supervision: null  # Deprecated. Here for backward compatibility, use translation instead. If not null, will override the value in translation.
    translation: 1.
    cosine: 0.
    contrastive: 1.

# =====
# Odd-one-out experiment parameters
# =====
odd_image:
  batch_size: 512
  encoder:
    selected_id: null
    path: null  # path to checkpoint. If null, will train a new encoder. If "random" will use a random encoder.
    # If "identity", will not use the encoder.
    # If a path is give, it can use ${odd_image.encoder.selected_slurm_id} which is automatically
    # set using the load_from below.
    # For example
    # path: "path/to/checkpoints/${odd_image.encoder.selected_id}/last.ckpt"
    # If the path leads to a folder, the last checkpoint is automatically loaded.

    load_from: null  # similar to the visualization. Can be null, "wandb" or "csv".
    # If "wandb", will override odd_image.encoder.selected_id from a run selected from wandb.
    # If "csv", will override from a run from the csv file given in odd_image.encoder.csv_path.
    # If null, will not overwrite odd_image.encoder.selected_id.
    wandb_entity_project: null
    wandb_filter: null
    csv_path: null
    selected_id_key: "Name"  # from which column the id is taken in the loaded run.

  # If load from is not null, which items to select from the loaded runs?
  select_row_from_index: null  # If this is not null, will select the corresponding item.
  select_row_from_current_coefficients: false  # if select_row_from_index is null and this is true, will select the run which uses the same loss coefficients as currently set.

  optimizer:
    lr: 3e-4
    weight_decay: 1e-4

downstream:
  unpaired_cls:
    random_regressor: false
    checkpoint: "path/to/checkpoint"

    optimizer:
      lr: 1e-3
      weight_decay: 1e-5

_code_version: master
